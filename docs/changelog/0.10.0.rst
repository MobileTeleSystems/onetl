0.10.0 (2023-12-18)
===================

Breaking Changes
----------------

- Upgrade ``etl-entities`` from v1 to v2 (:github:pull:`172`).

  This implies that ``HWM`` classes are now have different internal structure than they used to.

  Before:

  .. code-block:: python

    from etl_entities.old_hwm import IntHWM as OldIntHWM
    from etl_entities.source import Column, Table
    from etl_entities.process import Process

    OldIntHWM(
        column=Column(name="col1"),
        source=Table(name="schema.table", instance="postgres://host:5432/db"),
        process=Process(name="myprocess", task="abc", dag="cde", host="myhost"),
        value=123,
    )

  After:

  .. code-block:: python

    from etl_entities.hwm import ColumnIntHWM

    ColumnIntHWM(
        name="some_unique_name",
        description="any value you want",
        expression="col1",
        entity="schema.table",
        value=123,
    )

  **Breaking changes:**

  - If you used HWM classes from ``etl_entities`` module, you should rewrite your code to make it compatible with new version.

    .. dropdown:: More details

        - ``HWM`` classes used by previous onETL versions were moved from ``etl_entities`` to ``etl_entities.old_hwm`` submodule.
        - New ``HWM`` classes have flat structure instead of nested.
        - New ``HWM`` classes have mandatory ``name`` attribute (it was known as ``qualified_name`` before).
        - Type aliases used while serializing and deserializing ``HWM`` objects to ``dict`` representation were changed too: ``int`` -> ``column_int``.
        - HWM Store implementations now can handle only new ``HWM`` classes, old ones are **NOT** supported.

        To make migration simpler, you can use new method:

        .. code-block:: python

            old_hwm = OldIntHWM(...)
            new_hwm = old_hwm.as_new_hwm()

        Which automatically converts all fields from old structure to new one, including ``qualified_name`` -> ``name``.

  - **YAMLHWMStore** CANNOT read files created by older onETL versions (0.9.x or older).

- Classes:

  * ``BaseHWMStore``
  * ``HWMStoreClassRegistry``
  * ``register_hwm_store_class``
  * ``HWMStoreManager``
  * ``MemoryHWMStore``
  * ``detect_hwm_store``

  were moved from ``onetl.hwm.store`` to ``etl_entities.hwm_store`` module.

  They still can be imported from old module, but this is deprecated and will be removed in v1.0.0 release.

  **Breaking change:** methods ``BaseHWMStore.get()`` and ``BaseHWMStore.save()`` were renamed to ``get_hwm()`` and ``set_hwm``.

  If you used them in your code, please update it accordingly.

- Change the way of passing ``HWM`` to ``DBReader`` and ``FileDownloader`` classes:

  .. list-table::
    :header-rows: 1
    :widths: 30 30

    * - onETL ``0.9.x`` and older
      - onETL ``0.10.x`` and newer

    * -
        .. code-block:: python

            reader = DBReader(
                connection=...,
                source=...,
                hwm_column="col1",
            )

      -
        .. code-block:: python

            reader = DBReader(
                connection=...,
                source=...,
                hwm=DBReader.AutoDetectHWM(
                    # name is mandatory now!
                    name="my_unique_hwm_name",
                    expression="col1",
                ),
            )

    * -
        .. code-block:: python

            reader = DBReader(
                connection=...,
                source=...,
                hwm_column=(
                    "col1",
                    "cast(col1 as date)",
                ),
            )

      -
        .. code-block:: python

            reader = DBReader(
                connection=...,
                source=...,
                hwm=DBReader.AutoDetectHWM(
                    # name is mandatory now!
                    name="my_unique_hwm_name",
                    expression="cast(col1 as date)",
                ),
            )

    * -
        .. code-block:: python

            downloader = FileDownloader(
                connection=...,
                source_path=...,
                target_path=...,
                hwm_type="file_list",
            )

      -
        .. code-block:: python

            downloader = FileDownloader(
                connection=...,
                source_path=...,
                target_path=...,
                hwm=FileListHWM(
                    # name is mandatory now!
                    name="another_unique_hwm_name",
                ),
            )

  New HWM classes have **mandatory** ``name`` attribute which should be passed explicitly,
  instead of generating if automatically under the hood.

  Automatic ``name`` generation using the old ``DBReader.hwm_column`` / ``FileDownloader.hwm_type``
  syntax is still supported, but will be removed in v1.0.0 release. (:github:pull:`179`)

- Implementation of read strategies has been drastically improved. (:github:pull:`182`).

  ``DBReader.run()`` + incremental/batch strategy behavior in versions 0.9.x and older:

  - Get table schema by making query ``SELECT * FROM table WHERE 1=0`` (if ``DBReader.columns`` has ``*``)
  - Append HWM column to list of table columns and remove duplicated columns.
  - Create dataframe from query like ``SELECT hwm.expression AS hwm.column, ...other table columns... FROM table WHERE prev_hwm.expression > prev_hwm.value``.
  - Determine HWM class using dataframe schema: ``df.schema[hwm.column].dataType``.
  - Determine x HWM column value using Spark: ``df.select(min(hwm.column), max(hwm.column)).collect()``.
  - Use ``max(hwm.column)`` as next HWM value.
  - Return dataframe to user.

  This was far from ideal:

  - Dataframe content (all rows or just changed ones) was loaded from the source to Spark only to get min/max values of specific column.

  - Step of fetching table schema and then substituting column names in the next query caused some unexpected errors.

    .. dropdown:: Explanation

        For example, source contains columns with mixed name case, like ``"CamelColumn"`` or ``"spaced column"``.

        Column names were *not* escaped during query generation, leading to queries that cannot be executed by database.

        So users have to *explicitly* pass column names ``DBReader``, wrapping columns with mixed naming with ``"``:

        .. code:: python

            reader = DBReader(
                connection=...,
                source=...,
                columns=[  # passing '*' here leads to wrong SQL query generation
                    "normal_column",
                    '"CamelColumn"',
                    '"spaced column"',
                    ...,
                ],
            )

  - Using ``DBReader`` with ``IncrementalStrategy`` could lead to reading rows already read before.

    .. dropdown:: Explanation

        Dataframe was created from query with WHERE clause like ``hwm.expression > prev_hwm.value``,
        not ``hwm.expression > prev_hwm.value AND hwm.expression <= current_hwm.value``.

        So if new rows appeared in the source **after** HWM value is determined,
        they can be read by accessing dataframe content (because Spark dataframes are lazy),
        leading to inconsistencies between HWM value and dataframe content.

        This may lead to issues then ``DBReader.run()`` read some data, updated HWM value, and next call of ``DBReader.run()``
        will read rows that were already read in previous run.

  ``DBReader.run()`` + incremental/batch strategy behavior in versions 0.10.x and newer:

  - Get type of HWM expression: ``SELECT hwm.expression FROM table WHERE 1=0``.
  - Determine HWM class by accessing ``df.schema[0]``.
  - Get min/max values by querying the source: ``SELECT MIN(hwm.expression), MAX(hwm.expression) FROM table WHERE hwm.expression >= prev_hwm.value``.
  - Use ``max(hwm.expression)`` as next HWM value.
  - Create dataframe from query ``SELECT * FROM table WHERE hwm.expression > prev_hwm.value AND hwm.expression <= current_hwm.value``, baking new HWM value into the query.
  - Return dataframe to user.

  Improvements:

  - Allow source to calculate min/max instead of loading everything to Spark. This should be **faster** on large amounts of data, especially when source have indexes for HWM column.
  - Restrict dataframe content to always match HWM values, which leads to never reading the same row twice.
  - Columns list is passed to source as-is, without any resolving on ``DBReader`` side. So you can pass ``DBReader(columns=["*"])`` to read tables with mixed columns naming.

  **Breaking change**: HWM column is not being implicitly added to dataframe.

  So if you had code like this:

  .. code-block:: python

    reader = DBReader(
        connection=...,
        source=...,
        columns=["col1", "col2"],
        hwm_column="id",
    )

    df = reader.run()
    df.id  # accessing hwm_column value in the dataframe

    # or

    reader = DBReader(
        connection=...,
        source=...,
        columns=["col1", "col2"],
        hwm_column=("id", "cast(id as int)"),
    )

    df = reader.run()
    df.id  # accessing hwm_expression value in the dataframe

  you should rewrite it like this:

  .. code-block:: python

    reader = DBReader(
        connection=...,
        source=...,
        # add hwm_column explicitly to columns list
        columns=["id", "col1", "col2"],
        hwm_column="id",
    )

    df = reader.run()
    df.id  # accessing hwm_column value in the dataframe

    # or

    reader = DBReader(
        connection=...,
        source=...,
        # add hwm_column explicitly to columns list
        columns=["cast(id as int) as id", "col1", "col2"],
        hwm_column=("id", "cast(id as int)"),
    )

    df = reader.run()
    df.id  # accessing hwm_expression value in the dataframe

- ``FileDownloader.run()`` now updates HWM in HWM Store not after each file is being successfully downloaded,
  but after all files were handled.

  This is because:

  * FileDownloader can be used with ``DownloadOptions(workers=N)``, which could lead to race condition - one thread can reset HWM value which is updated by another thread.
  * FileDownloader can download hundreds and thousands of files, and issuing a request to HWM Store for each file could potentially DDoS HWM Store. (:github:pull:`189`)


Features
--------

- Add Python 3.12 compatibility. (:github:pull:`167`)
- Add ``DBReader.AutoHWM`` class for auto-detecting hwm class by source column type (:github:pull:`179`)
- Update ``Excel`` package version to 0.20.3. (:github:pull:`187`)
- ``SnapshotBatchStagy`` and ``IncrementalBatchStrategy`` does no raise exceptions if source does not contain any data.
  Instead they stop at first iteration and return empty dataframe. (:github:pull:`188`)
- Cache result of ``connection.check()`` in high-level classes like ``DBReader``, ``FileDownloader`` and so on. This makes logs less verbose. (:github:pull:`190`)

Bug Fixes
---------

- Fix ``@slot`` and ``@hook`` decorators returning methods with missing arguments in signature (Pylance, VS Code). (:github:pull:`183`)
- Kafka connector documentation said that it does support reading topic data incrementally by passing ``group.id`` or ``groupIdPrefix``.
  Actually, this is not true, because Spark does not send information to Kafka which messages were consumed.
  So currently users can only read the whole topic, no incremental reads are supported.

name: Tests for Kafka
on:
  workflow_call:
    inputs:
      kafka-version:
        required: true
        type: string
      spark-version:
        required: true
        type: string
      pydantic-version:
        required: true
        type: string
      java-version:
        required: true
        type: string
      python-version:
        required: true
        type: string
      os:
        required: true
        type: string
      with-cache:
        required: false
        type: boolean
        default: true

jobs:
  test-kafka:
    name: Run Kafka tests (server=${{ inputs.kafka-version }}, spark=${{ inputs.spark-version }}, pydantic=${{ inputs.pydantic-version }}, java=${{ inputs.java-version }}, python=${{ inputs.python-version }}, os=${{ inputs.os }})
    runs-on: ${{ inputs.os }}

    services:
      zookeeper:
        image: bitnami/zookeeper:3.8
        env:
          TZ: UTC
          ALLOW_ANONYMOUS_LOGIN: 'yes'
        options: >-
          --health-cmd "nc -z localhost 2181 || exit"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      kafka:
        image: bitnami/kafka:${{ inputs.kafka-version }}
        env:
          TZ: UTC
          ALLOW_PLAINTEXT_LISTENER: 'yes'
          KAFKA_ENABLE_KRAFT: 'no'
          KAFKA_CLIENT_USERS: onetl
          KAFKA_CLIENT_PASSWORDS: 123UsedForTestOnly@!
          KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
          KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_ZOOKEEPER_BOOTSTRAP_SCRAM_USERS: 'true'
          KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL_PLAINTEXT_ANONYMOUS
          KAFKA_CFG_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://:9093,INTERNAL_PLAINTEXT_SASL://:9094,EXTERNAL_PLAINTEXT_SASL://:9095
          KAFKA_CFG_ADVERTISED_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://kafka:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://localhost:9093,INTERNAL_PLAINTEXT_SASL://kafka:9094,EXTERNAL_PLAINTEXT_SASL://localhost:9095
          KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT,EXTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT,INTERNAL_PLAINTEXT_SASL:SASL_PLAINTEXT,EXTERNAL_PLAINTEXT_SASL:SASL_PLAINTEXT
          KAFKA_CFG_SASL_ENABLED_MECHANISMS: PLAIN,SCRAM-SHA-256,SCRAM-SHA-512
        ports:
          - 9093:9093
          - 9095:9095
        options: >-
          --health-cmd "kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java ${{ inputs.java-version }}
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ inputs.java-version }}

      - name: Set up Python ${{ inputs.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}

      - name: Cache Ivy
        uses: actions/cache@v4
        if: inputs.with-cache
        with:
          path: ~/.ivy2
          key: ${{ runner.os }}-ivy-${{ inputs.spark-version }}-${{ inputs.pydantic-version }}-tests-kafka-${{ hashFiles('onetl/connection/db_connection/*.py', 'onetl/connection/file_df_connection/*.py') }}
          restore-keys: |
            ${{ runner.os }}-ivy-${{ inputs.spark-version }}-${{ inputs.pydantic-version }}-tests-kafka-${{ hashFiles('onetl/connection/db_connection/*.py', 'onetl/connection/file_df_connection/*.py') }}
            ${{ runner.os }}-ivy-${{ inputs.spark-version }}-${{ inputs.pydantic-version }}-tests-kafka-

      - name: Cache pip
        uses: actions/cache@v4
        if: inputs.with-cache
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-spark-${{ hashFiles('requirements/core.txt', 'requirements/tests/base.txt', 'requirements/tests/spark-*.txt', 'requirements/tests/pydantic-*.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-spark-${{ hashFiles('requirements/core.txt', 'requirements/tests/base.txt', 'requirements/tests/spark-*.txt', 'requirements/tests/pydantic-*.txt') }}
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-spark-

      - name: Upgrade pip
        run: python -m pip install --upgrade pip setuptools wheel

      - name: Install dependencies
        run: |
          pip install -I -r requirements/core.txt -r requirements/tests/base.txt -r requirements/tests/kafka.txt -r requirements/tests/spark-${{ inputs.spark-version }}.txt -r requirements/tests/pydantic-${{ inputs.pydantic-version }}.txt

      - name: Run tests
        run: |
          mkdir reports/ || echo "Directory exists"
          sed '/^$/d' ./.env.local | sed '/^#/d' | sed 's/^/export /' > ./env
          source ./env
          ./pytest_runner.sh -m kafka

      - name: Dump Kafka logs on failure
        if: failure()
        uses: jwalton/gh-docker-logs@v2
        with:
          images: bitnami/kafka
          dest: ./logs

      - name: Upload Kafka logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: container-logs-kafka-${{ inputs.kafka-version }}-spark-${{ inputs.spark-version }}-python-${{ inputs.python-version }}-os-${{ inputs.os }}
          path: logs/*

      - name: Upload coverage results
        uses: actions/upload-artifact@v4
        with:
          name: coverage-kafka-${{ inputs.kafka-version }}-spark-${{ inputs.spark-version }}-python-${{ inputs.python-version }}-os-${{ inputs.os }}
          path: reports/*
          # https://github.com/actions/upload-artifact/issues/602
          include-hidden-files: true

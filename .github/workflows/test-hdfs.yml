name: Tests for HDFS
on:
  workflow_call:
    inputs:
      hadoop-version:
        required: true
        type: string
      spark-version:
        required: true
        type: string
      pydantic-version:
        required: true
        type: string
      java-version:
        required: true
        type: string
      python-version:
        required: true
        type: string
      os:
        required: true
        type: string
      with-cache:
        required: false
        type: boolean
        default: true

jobs:
  test-hdfs:
    name: Run HDFS tests (server=${{ inputs.hadoop-version }}, spark=${{ inputs.spark-version }}, pydantic=${{ inputs.pydantic-version }}, java=${{ inputs.java-version }}, python=${{ inputs.python-version }}, os=${{ inputs.os }})
    runs-on: ${{ inputs.os }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Start HDFS
        run: |
          docker compose --profile all down -v --remove-orphans
          docker compose --profile hdfs pull
          docker compose --profile hdfs up -d --wait --wait-timeout 200

      - name: Set up Java ${{ inputs.java-version }}
        uses: actions/setup-java@v5
        with:
          distribution: temurin
          java-version: ${{ inputs.java-version }}

      - name: Set up Python ${{ inputs.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ inputs.python-version }}

      - name: Set up Kerberos libs
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install --no-install-recommends libkrb5-dev gcc

      - name: Cache pip
        uses: actions/cache@v4
        if: inputs.with-cache
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-hdfs-${{ hashFiles('requirements/core.txt', 'requirements/tests/base.txt', 'requirements/tests/spark-*.txt', 'requirements/tests/pydantic-*.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-hdfs-${{ hashFiles('requirements/core.txt', 'requirements/tests/base.txt', 'requirements/tests/spark-*.txt', 'requirements/tests/pydantic-*.txt') }}
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-spark-${{ hashFiles('requirements/core.txt', 'requirements/tests/base.txt', 'requirements/tests/spark-*.txt', 'requirements/tests/pydantic-*.txt') }}
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-hdfs-
            ${{ runner.os }}-python-${{ inputs.python-version }}-spark-${{ inputs.spark-version }}-pydantic-${{ inputs.pydantic-version }}-tests-spark-

      - name: Upgrade pip
        run: python -m pip install --upgrade pip setuptools wheel

      - name: Install dependencies
        run: |
          pip install -I -r requirements/core.txt -r requirements/kerberos.txt -r requirements/hdfs.txt -r requirements/tests/base.txt -r requirements/tests/spark-${{ inputs.spark-version }}.txt -r requirements/tests/pydantic-${{ inputs.pydantic-version }}.txt
        env:
          HDFS_IMAGE: mtsrus/hadoop:${{ inputs.hadoop-version }}

      - name: Run tests
        run: |
          mkdir reports/ || echo "Directory exists"
          source .env.local
          echo "127.0.0.1 hdfs" | sudo tee -a /etc/hosts
          ./pytest_runner.sh -m hdfs

      - name: Dump HDFS logs on failure
        if: failure()
        uses: jwalton/gh-docker-logs@v2
        with:
          images: mtsrus/hadoop
          dest: ./logs

      - name: Upload HDFS logs
        uses: actions/upload-artifact@v5
        if: failure()
        with:
          name: container-logs-hdfs-${{ inputs.hadoop-version }}-spark-${{ inputs.spark-version }}-python-${{ inputs.python-version }}-os-${{ inputs.os }}
          path: logs/*

      - name: Shutdown HDFS
        if: always()
        run: |
          docker compose --profile all down -v --remove-orphans

      - name: Upload coverage results
        uses: actions/upload-artifact@v5
        with:
          name: coverage-hdfs-${{ inputs.hadoop-version }}-spark-${{ inputs.spark-version }}-python-${{ inputs.python-version }}-os-${{ inputs.os }}
          path: reports/*
          # https://github.com/actions/upload-artifact/issues/602
          include-hidden-files: true

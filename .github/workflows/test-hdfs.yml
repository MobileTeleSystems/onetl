name: Tests for HDFS
on:
  workflow_call:
    inputs:
      hadoop-version:
        required: true
        type: string
      python-version:
        required: true
        type: string
      os:
        required: true
        type: string

jobs:
  test-hdfs:
    name: Run HDFS tests (server=${{ inputs.hadoop-version }}, python=${{ inputs.python-version }}, os=${{ inputs.os }})
    runs-on: ${{ inputs.os }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python ${{ inputs.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.python-version }}

    - name: Set up Kerberos libs
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update && sudo apt-get install --no-install-recommends libkrb5-dev gcc

    - name: Cache pip
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-python-${{ inputs.python-version }}-tests-hdfs-${{ hashFiles('requirements/core.txt', 'requirements/kerberos.txt', 'requirements/hdfs.txt', 'requirements/tests/base.txt', 'requirements/tests/spark*.txt') }}
        restore-keys: |
          ${{ runner.os }}-python-${{ inputs.python-version }}-tests-hdfs-${{ hashFiles('requirements/core.txt', 'requirements/kerberos.txt', 'requirements/hdfs.txt', 'requirements/tests/base.txt', 'requirements/tests/spark*.txt') }}
          ${{ runner.os }}-python-${{ inputs.python-version }}-tests-hdfs-

    - name: Upgrade pip
      run: python -m pip install --upgrade pip setuptools wheel

    - name: Install dependencies
      run: |
        pip install -I \
          -r requirements/core.txt \
          -r requirements/kerberos.txt \
          -r requirements/hdfs.txt \
          -r requirements/tests/base.txt

    # Cannot use services because we need to mount config file from the repo, but services start before checkout.
    # See https://github.com/orgs/community/discussions/25792
    - name: Start HDFS
      run: |
        docker compose down -v --remove-orphans
        docker compose up -d hdfs --wait --wait-timeout 200 &
        wait_pid=$!
        docker compose logs -f hdfs &
        wait $wait_pid
      env:
        HDFS_IMAGE: mtsrus/hadoop:${{ inputs.hadoop-version }}
        COMPOSE_PROJECT_NAME: ${{ github.run_id }}-hadoop${{ inputs.hadoop-version }}

    - name: Wait for HDFS to be ready
      run: |
        ./docker/wait-for-it.sh -h localhost -p 9870 -t 60

    - name: Run tests
      run: |
        mkdir reports/ || echo "Directory exists"
        sed '/^$/d' ./.env.local | sed '/^#/d' | sed 's/^/export /' > ./env
        source ./env
        echo "127.0.0.1 hdfs" | sudo tee -a /etc/hosts
        ./pytest_runner.sh -m hdfs

    - name: Shutdown HDFS
      if: always()
      run: |
        docker compose down -v --remove-orphans
      env:
        COMPOSE_PROJECT_NAME: ${{ github.run_id }}-hadoop${{ inputs.hadoop-version }}

    - name: Upload coverage results
      uses: actions/upload-artifact@v3
      with:
        name: hdfs-${{ inputs.hadoop-version }}-python-${{ inputs.python-version }}-os-${{ inputs.os }}
        path: reports/*

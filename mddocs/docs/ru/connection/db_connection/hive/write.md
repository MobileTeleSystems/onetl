# Запись данных в Hive с помощью `DBWriter` { #hive-write }

Для записи данных в Hive используйте [DBWriter][db-writer].

## Примеры

    ```python
        from onetl.connection import Hive
        from onetl.db import DBWriter

        hive = Hive(...)

        df = ...  # данные находятся здесь

        # Создаем датафрейм с определенным количеством партиций Spark.
        # Используем колонки партиционирования Hive для группировки данных. Создаем максимум 20 файлов на каждую партицию Hive.
        # Также сортируем данные по колонке, с которой наиболее коррелируют данные (например, user_id), уменьшая размер файлов.

        num_files_per_partition = 20
        partition_columns = ["country", "business_date"]
        sort_columns = ["user_id"]
        write_df = df.repartition(
            num_files_per_partition,
            *partition_columns,
            *sort_columns,
        ).sortWithinPartitions(*partition_columns, *sort_columns)

        writer = DBWriter(
            connection=hive,
            target="schema.table",
            options=Hive.WriteOptions(
                if_exists="append",
                # Колонки партиционирования Hive.
                partitionBy=partition_columns,
            ),
        )

        writer.run(write_df)
    ```

## Рекомендации

### Используйте колоночные форматы записи

Предпочтительные форматы записи:

- [ORC](https://spark.apache.org/docs/latest/sql-data-sources-orc.html) (**по умолчанию**)
- [Parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)
- [Iceberg](https://iceberg.apache.org/spark-quickstart/)
- [Hudi](https://hudi.apache.org/docs/quick-start-guide/)
- [Delta](https://docs.delta.io/latest/quick-start.html#set-up-apache-spark-with-delta-lake)

!!! warning
    При использовании `DBWriter` формат данных spark по умолчанию, настроенный в `spark.sql.sources.default`, игнорируется, так как значение по умолчанию для `Hive.WriteOptions(format=...)` явно установлено на `orc`.

В колоночных форматах записи каждый файл содержит отдельные секции, где хранятся данные столбцов. Подвал файла содержит
информацию о расположении каждой секции/группы столбцов. Spark может использовать эту информацию для загрузки только тех секций, которые требуются для конкретного запроса, например, только выбранных столбцов, чтобы значительно ускорить выполнение запроса.

Еще одно преимущество — высокий коэффициент сжатия, например, в 10-100 раз по сравнению с JSON или CSV.

### Используйте партиционирование

#### Как это работает

Hive поддерживает разделение данных на партиции, которые представляют собой разные каталоги в файловой системе с именами типа `some_col=value1/another_col=value2`.

Например, датафрейм с содержимым:

| country: string | business_date: date | user_id: int | bytes: long |
| --------------- | ------------------- | ------------ | ----------- |
| RU              | 2024-01-01          | 1234         | 25325253525 |
| RU              | 2024-01-01          | 2345         | 23234535243 |
| RU              | 2024-01-02          | 1234         | 62346634564 |
| US              | 2024-01-01          | 5678         | 4252345354  |
| US              | 2024-01-02          | 5678         | 5474575745  |
| US              | 2024-01-03          | 5678         | 3464574567  |

С параметром `partitionBy=["country", "business_dt"]` данные будут храниться в виде файлов в следующих подкаталогах:

- `/country=RU/business_date=2024-01-01/`
- `/country=RU/business_date=2024-01-02/`
- `/country=US/business_date=2024-01-01/`
- `/country=US/business_date=2024-01-02/`
- `/country=US/business_date=2024-01-03/`

Отдельный подкаталог создается для каждой уникальной комбинации значений столбцов в датафрейме.

Пожалуйста, не путайте партиции датафрейма Spark (т.е. партии данных, обрабатываемые исполнителями Spark, обычно параллельно) и партиционирование Hive (хранение данных в разных подкаталогах).
Количество партиций датафрейма Spark коррелирует с количеством файлов, создаваемых в **каждой** партиции Hive.
Например, датафрейм Spark с 10 партициями и 5 различными значениями столбцов партиции Hive будет сохранен как 5 подкаталогов по 10 файлов в каждом = всего 50 файлов.
Без партиционирования Hive все файлы размещаются в одном плоском каталоге.

#### Но зачем?

Запросы, которые имеют условие `WHERE` с фильтрами по столбцам партиционирования Hive, например `WHERE country = 'RU' AND business_date='2024-01-01'`, будут считывать только файлы из этой конкретной партиции, например `/country=RU/business_date=2024-01-01/`, и пропускать файлы из других партиций.

Это значительно повышает производительность и снижает объем памяти, используемой Spark.
Рекомендуется использовать партиционирование Hive во всех таблицах.

#### Какие колонки следует использовать?

Обычно столбцы партиционирования Hive основываются на дате события или местоположении, например `country: string`, `business_date: date`, `run_date: date` и т. д.

**Столбцы партиций должны содержать данные с низкой кардинальностью.**
Даты, небольшие целые числа, строки с малым количеством возможных значений подходят.
Но временные метки, числа с плавающей точкой, десятичные числа, длинные целые (например, идентификатор пользователя), строки с большим количеством уникальных значений (например, имя пользователя или email) **НЕ** должны использоваться в качестве столбцов партиционирования Hive.
В отличие от некоторых других баз данных, партиции на основе диапазонов и хэш-функций не поддерживаются.

Столбец партиции должен быть частью датафрейма. Если вы хотите разбить значения по компоненту даты столбца `business_dt: timestamp`, добавьте новый столбец в датафрейм следующим образом: `df.withColumn("business_date", date(df.business_dt))`.

### Используйте сжатие

Использование алгоритмов сжатия, таких как `snappy`, `lz4` или `zstd`, может уменьшить размер файлов (до 10 раз).

### Предпочитайте создавать большие файлы

HDFS и S3 не предназначены для хранения миллионов маленьких файлов. Минимальный размер файла должен быть не менее 10Мб, но обычно он составляет 128Мб+ или 256Мб+ (размер блока HDFS).
**НИКОГДА** не создавайте файлы размером в несколько килобайт.

Количество файлов может различаться в разных случаях.
С одной стороны, Spark Adaptive Query Execution (AQE) может объединять маленькие партиции датафрейма Spark в одну большую.
С другой стороны, датафреймы с данными, в которых есть перекос могут создавать больше файлов, чем ожидалось.

Для создания небольшого количества больших файлов можно уменьшить количество партиций датафрейма Spark.
Используйте функцию [df.repartition(N, columns...)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html), например: `df.repartition(20, "col1", "col2")`.
Это создаст новый датафрейм Spark с партициями, используя выражение `hash(df.col1 + df.col2) mod 20`, избегая перекоса данных.

**Примечание:** большие партиции датафрейма требуют больше ресурсов (CPU, RAM) на исполнителе Spark. Точное количество партиций
должно определяться эмпирически, так как оно зависит от объема данных и доступных ресурсов.

### Сортируйте данные перед записью

Датафрейм с отсортированным содержимым:

| country: string | business_date: date | user_id: int | business_dt: timestamp  | bytes: long |
| --------------- | ------------------- | ------------ | ----------------------- | ----------- |
| RU              | 2024-01-01          | 1234         | 2024-01-01T11:22:33.456 | 25325253525 |
| RU              | 2024-01-01          | 1234         | 2024-01-01T12:23:44.567 | 25325253525 |
| RU              | 2024-01-02          | 1234         | 2024-01-01T13:25:56.789 | 34335645635 |
| US              | 2024-01-01          | 2345         | 2024-01-01T10:00:00.000 | 12341       |
| US              | 2024-01-02          | 2345         | 2024-01-01T15:11:22.345 | 13435       |
| US              | 2024-01-03          | 2345         | 2024-01-01T20:22:33.567 | 14564       |

Имеет гораздо лучший коэффициент сжатия, чем несортированный, например, в 2 или даже больше раз:

| country: string | business_date: date | user_id: int | business_dt: timestamp  | bytes: long |
| --------------- | ------------------- | ------------ | ----------------------- | ----------- |
| RU              | 2024-01-01          | 1234         | 2024-01-01T11:22:33.456 | 25325253525 |
| RU              | 2024-01-01          | 6345         | 2024-12-01T23:03:44.567 | 25365       |
| RU              | 2024-01-02          | 5234         | 2024-07-01T06:10:56.789 | 45643456747 |
| US              | 2024-01-01          | 4582         | 2024-04-01T17:59:00.000 | 362546475   |
| US              | 2024-01-02          | 2345         | 2024-09-01T04:24:22.345 | 3235        |
| US              | 2024-01-03          | 3575         | 2024-03-01T21:37:33.567 | 346345764   |

Выбор столбцов для сортировки данных действительно зависит от самих данных. Если данные коррелируют с каким-то конкретным столбцом, как в примере выше, где объем трафика коррелирует как с `user_id`, так и с `timestamp`, используйте `df.sortWithinPartitions("user_id", "timestamp")` перед записью данных.

Если `df.repartition(N, repartition_columns...)` используется в сочетании с `df.sortWithinPartitions(sort_columns...)`, то `sort_columns` должны начинаться с `repartition_columns` или быть равными им.

## Опции

<!-- 
```{eval-rst}
.. currentmodule:: onetl.connection.db_connection.hive.options
```

```{eval-rst}
.. autopydantic_model:: HiveWriteOptions
    :member-order: bysource
    :model-show-field-summary: false
    :field-show-constraints: false
``` 
-->

::: onetl.connection.db_connection.hive.options.HiveWriteOptions
    options:
        inherited_members: true
        heading_level: 3
        show_root_heading: true
